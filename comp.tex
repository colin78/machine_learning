%!TEX root = paper.tex

\section{Computational Results}\label{sec:comp}

We compare the training and test set accuracy for all of the simulated datasets, which are summarized in Table~\ref{tab:all_acc}.
The predictive function for standard logistic regression is simple and the predictive function for MFVB logistic regression has already been discussed in Section~\ref{sec:implement}.  However, the predictive function for MCMC is new, and we will describe here our method.  To predict the label $P(y=1|\M{x})$ for MCMC, we use the predictive function for MFVB logistic regression, except substituting the empirical mean $\M{\bar x}$ and empirical covariance ${\hat \Sigma}$ in place of the MFVB mean $\M{w}_N$ and standard deviation $\M{V}_N$.
 
Overall, we see that MCMC yields the highest out-of-sample accuracy in all cases except for Dataset 9.  (In Dataset 7, no result for
MCMC was reported because the data was separable.)  Logistic Regression is the second strongest method, and for most datasets logistic regression yields out-of-sample accuracy results matching MCMC.  Here, MFVB performs worse than the other methods, which is expected because MFVB requires the most restrictive set of assumptions: namely that the joint distribution factorizes into independent marginals.  Because MFVB is an approximate method, it much runs faster than MCMC, but as a result may sacrifice some predictive power.  For larger problems with many observations $n$ and high-dimension $p$, this tradeoff between computational complexity and performance may become more severe.  


\input{results/all_accuracy}